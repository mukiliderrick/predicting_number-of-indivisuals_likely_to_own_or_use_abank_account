# -*- coding: utf-8 -*-
"""moringa_data_science_core_core_w3_independent _project_2020_03_Derrick_Mukili_python

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RDLGJ7eUzyKQQzBxWzRi7XjQw6w6dBKJ

# RESEARCH QUESTION 

Traditionally, access to bank accounts has been regarded as an indicator of financial inclusion. Despite the proliferation of mobile money in Africa and the growth of innovative fintech solutions, banks still play a pivotal role in facilitating access to financial services. Access to bank accounts enables households to save and facilitate payments while also helping businesses build up their credit-worthiness and improve their access to other financial services. Therefore, access to bank accounts is an essential contributor to long-term economic growth.

.1.Find out individuals who are more likely to own a bank account
2. find the key attributes that make an individual own a bank account

# Univariate Analysis with Python

## 1.0 Importing Libraries
"""

# Importing Pandas
# 
import pandas as pd
# Importing Numpy
#
import numpy as np
# Importing Seaborn
#
import seaborn as sns
# Importing Matplotlib
#
import matplotlib.pyplot as plt

"""#loading our Data"""

#loading our dataset
#thenviewing the first five rows at the top
financial = pd.read_csv('/content/Financial Dataset - 1 (1).csv')
financial.head()

"""## viewing our data"""

#checking the number of records our data has
financial.shape

#checking if our data is with the appropriate data types
financial.dtypes

"""#Tiding our data"""

#viewing out column names that we are remaning
financial.columns

#checking for null values
financial.isnull().sum()

#droppimg null values as they will affect our analysis negatively
fin1= financial.dropna()

#checking if the null values have been dropped
fin1.isnull().sum()

# Dropping columns we do not need for this analysis

fino = fin1.drop(['uniqueid'], axis = 1)

#checking if uniqueId is still within the dataset
fino.columns

"""##Fixing column names"""

#some column names has error of misspelled words
fino.rename(columns={'level of education':'level_of_education'}, inplace=True)#fixing level of education name
fino.rename(columns={'The relathip with head':'the_relationship_with_head'}, inplace=True)#fixing the relationship with head name

#confirming if the column names have been corrected
fino.columns

fin = fino

"""## Checking for outliers"""

#checking for outliers
# we can see that we do have outliers in our dataset
fig, (ax1,ax2) = plt.subplots(1,2, figsize=(10,7))
fig.suptitle('plot of outliers')
sns.boxplot(fino['household_size'],ax=ax1)
sns.boxplot(fino['Respondent Age'], ax=ax2)
plt.show()

"""# checking for anomalies"""

#removing the outliers in the two columns
q1 = fino.quantile(.25)
q3 = fino.quantile(.75)

iqr =q3 -q1
print(iqr)

"""#removing outliers"""

fin= fino[~((fino < (q1 - 1.5 * iqr)) |(fino > (q3 + 1.5 * iqr))).any(axis=1)]
print(fin.shape)

#viewinf the  year column for anomalies
fin['year'].unique()

#remocing the anomalies from the year colums
anom = fin[fin['year']> 2018].index
fin.drop(anom,inplace=True)

"""#Exploratory Data Analysis

##   Univariate **analysis**

###Bar charts
"""

#plotting abar graph of participants with bank accounts vs those who dont
#majority of the individuals do not own bank accounts
plt.figure(dpi = 100)
sns.countplot(fin['Has a Bank account'])
plt.title("A bar chart that shows people with bank account")
plt.show()

"""### Histogram"""

#a histogram showing the household size distribution
plt.figure(dpi = 100)
sns.distplot(fin['household_size'], color= 'green')
plt.title('A histogram chart of the house hold size')
plt.show()

#a histogram showing the distribution of the Population age
plt.figure(dpi = 100)
sns.distplot(fin['Respondent Age'], color= 'green')
plt.title('A histogram chart of Respondent age')
plt.show()

"""###     Pie charts"""

# A pie chart showing the distribution of individuals with bank accounts per country
#kenya has the majority of people with bank accounts
bank =fin.groupby(['country'])['Has a Bank account'].count()
print(bank)
label = ['Rwanda','Kenya','Tanzania','Uganda']
label

plt.figure(figsize=(5,5),dpi=100)
plt.pie(bank, labels = label, autopct= '%1.1f%%', shadow= True,  startangle= 30)
plt.axis('equal')
plt.title('A Pie chart with a summary of people with bank account', color='gold')
plt.show()

# A pie chart showing the distribution of individuals with bank accounts per Gender
bank =fin.groupby(['gender_of_respondent'])['Has a Bank account'].count()
print(bank)
label = ['Male','Female']
label

plt.figure(figsize=(5,5),dpi=100)
plt.pie(bank, labels = label, autopct= '%1.1f%%', shadow= True,  startangle= 30)
plt.axis('equal')
plt.title('A Pie chart with a summary of people with bank account', color='gold')
plt.show()

"""### Central Tendancy"""

#checking the mean for the numerical data that is year, household size and respondent age
print('house houlds mean',fin['household_size'].mean())
print('respomdent age mean',fin['Respondent Age'].mean())

#checking the mode for the numerical data that is year, household size and respondent age
print('house houlds median',fin['household_size'].mode())
print('respomdent age median',fin['Respondent Age'].mode())

#checking the mode for the numerical data that is year, household size and respondent age
print('house houlds median',fin['household_size'].median())
print('respomdent age median',fin['Respondent Age'].median())

"""##Bivariate analysis

###Scatter plots
"""

# we are going to plot a scatter plot so that we can see is their is a correlation between the householdsize and the respondent age
#this shows that their is no relationship between the respondent age and athe household size as we can not see any positive or negative trend
plt.figure(dpi= 100)
plt.scatter(fin['household_size'], fin['Respondent Age'])
plt.title('A scatter plot of house hold size against the age', color = 'gold')
plt.xlabel('household size')
plt.ylabel('respondent age')
plt.show()

"""### Line graph"""

#their are too many variable to plot
plt.figure(figsize = (10,10), dpi = 100)
plt.plot(fin[['Respondent Age', 'household_size']])
plt.xticks(rotation = 45)
plt.title('A line chart of respondent age and household size', color = 'Green')
plt.xlabel('respondent age')
plt.ylabel('household size')
plt.show()

#we are going to limit the number of rows
find=fin.iloc[0:20,-7:-5]
plt.figure(figsize = (10,10), dpi = 100)
plt.plot(find[['Respondent Age', 'household_size']])
plt.xticks(rotation = 45)
plt.title('A line chart of respondent age and household size', color = 'Green')
plt.xlabel('respondent age')
plt.ylabel('household size')
plt.show()

"""###Heat map"""

#this is a heat map showing the realationship between the year,householdsize and the respondent age
sns.heatmap(fin.corr(), annot=True)
plt.show()

"""# MULTIVARIATE

## Principal Component Analysis (PCA)

## mapping of the the categorical variables with numerical data
"""

map_country ={
    "Kenya": 0,
    "Rwanda": 1,
    "Tanzania": 2,
    "Uganda": 3
}

fin['country']=fin['country'].map(map_country)

map_bank ={
    "Yes": 1,
    "No":0    
}
fin['Has a Bank account']=fin['Has a Bank account'].map(map_bank)

map_location ={
    "Rural": 1,
    "Urban":0
}
fin['Type of Location']=fin['Type of Location'].map(map_location)

map_phone ={
    "Yes": 1,
    "No":0
    
}
fin['Cell Phone Access']=fin['Cell Phone Access'].map(map_phone)

map_gender={
    'Female':1,
    'Male':0
}
fin['gender_of_respondent'] =fin['gender_of_respondent'].map(map_gender)

map_head={
    "Spouse":2,
    "Head of Household":1,
    "Other relative":0,
    "Child":3,
    "Parent":4,
    "Other non-relatives":5
}
fin['the_relationship_with_head']=fin['the_relationship_with_head'].map(map_head)

map_education={
    "No formal education":0,
    "Primary education":1,
    "Secondary education":2,
    "Vocational/Specialised training":3,
    "Tertiary education":4,
    "Other/Dont know/RTA":5,
    "6":6
}
fin['Level of Educuation']=fin['Level of Educuation'].map(map_education)

map_job={
    "Self employed":0,
    "Government Dependent":1,
    "Formally employed Private":2,
    "Informally employed":3,
    "Formally employed Government":4,
    "Farming and Fishing":5,
    "Remittance Dependent":6,
    "Other Income":7,
    "Dont Know/Refuse to answer":8,
    "No Income":9
}
fin['Type of Job']=fin['Type of Job'].map(map_job)

#checking our data to see if the mapping worked
fin.head()

#preprocessing step
#we are diving the data into the labels which is the y- axis and the features as the x-axis
X = fin.drop(columns = ['year','Has a Bank account','marital_status'])
y = fin['Has a Bank account']

#we are going to split the data into a training and and a test data using the train_test_split 
#we are importing train_test_split from the skleanmodel
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.2, random_state =0)

#we are going to perform a normalization of our data
#
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.fit_transform(X_test)

#we are going to apply PCA
from sklearn.decomposition import PCA
pca = PCA(0.95)
X_train = pca.fit_transform(X_train)
X_test = pca.transform(X_test)

#explained variance ratio obtained from PCA
explained_variance = pca.explained_variance_ratio_

explained_variance

#we are going to use 3 principal componrnts to train our algorithm
from sklearn.decomposition import PCA 
pca = PCA(n_components=2)
X_train = pca.fit_transform(X_train)
X_test = pca.transform(X_test)

#we are training and making predictions
from sklearn.ensemble import RandomForestClassifier

classifier = RandomForestClassifier(max_depth = 2, random_state = 0)
classifier.fit(X_train, y_train)

# predicting the Test set results

y_pred = classifier.predict(X_test)

#we are going to have a performance evaluation
#we are going to use a confusin matrix for this
#we are getting a 86% accuracy from 4038 instances
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score

cm = confusion_matrix(y_test, y_pred)
print(cm)
print('Accuracy' , accuracy_score(y_test, y_pred))

"""## Linear Discriminant Analysis"""

#we are going to divid our data into two again
X_l = fin.drop(columns = ['year','Has a Bank account','marital_status'])
y_l = fin['Has a Bank account']

# wea re going to divide the data into training and and test sets
from sklearn.model_selection import train_test_split
X_l_train, X_l_test, y_l_train, y_l_test = train_test_split(X_l, y_l, test_size=0.2, random_state=0)

#normalizing our data using the standard scaler
#this is a feature scaling method
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_l_train = sc.fit_transform(X_l_train)
X_l_test = sc.transform(X_l_test)

#performing Linear Discrimin antAnalysis
#we are going to use only one component
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
lda = LDA(n_components=1)
X_l_train = lda.fit_transform(X_l_train, y_l_train)
X_l_test = lda.transform(X_l_test)

#training and making predictions
#we are going to use the RandomForestClassifier algorithm
from sklearn.ensemble import RandomForestClassifier

classifier = RandomForestClassifier(max_depth=2, random_state=0)
classifier.fit(X_l_train, y_l_train)
y_l_pred = classifier.predict(X_l_test)

#perfoming an evaluation using a confusion matrix
#we are getting 85% accuaracy.
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score

cm = confusion_matrix(y_l_test, y_l_pred)
print(cm)
print('Accuracy' + str(accuracy_score(y_l_test, y_l_pred)))

"""## Factor Analysis"""

fin.head()

#dropping unnecessy columns
fin.drop(['year','marital_status'], axis=1, inplace=True)

#checking if the columns were sropped
fin.head()

#installing factor analyzer
pip install factor_analyzer==0.2.3

#chi_scure
from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity
chi_square_value, p_value = calculate_bartlett_sphericity(fin)
chi_square_value,p_value
# the test was significant becouse the p-value is 0 which tells us that the correlation matrix is not an identy matrix

#choosing the number of factors
#here we can only see 3-factor analyzer are greater than 1 which means we only need to consider the three factors
from factor_analyzer import  FactorAnalyzer
fa = FactorAnalyzer()
fa.analyze(fin, 10, rotation=None)

ev, v = fa.get_eigenvalues()
ev

#the scree plot that drows a straight line for easch factor and its eigen values.

plt.scatter(range(1,fin.shape[1]+1),ev)
plt.plot(range(1,fin.shape[1]+1),ev)
plt.title('Scree Plot')
plt.xlabel('Factors')
plt.ylabel('Eigenvalue')
plt.grid()
plt.show()

#performing Factor Analysis
fa = FactorAnalyzer()
fa.analyze(fin,3, rotation='varimax')
fa.loadings

#performing factor analysis
fa = FactorAnalyzer()
fa.analyze(fin, 2, rotation="varimax")
fa.loadings

# Getting variance of each factors
fa.get_factor_variance()

